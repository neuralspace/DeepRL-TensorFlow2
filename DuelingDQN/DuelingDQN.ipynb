{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd4e84b-b321-4581-bbbf-8879b4613f74",
   "metadata": {},
   "source": [
    "# Dueling Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7f447-a15d-4ce2-a62e-a3912e13f72d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e040ab-9999-4ddf-a5d1-51c2b1594789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
    "    !pip install -q -U tf-agents pyvirtualdisplay gym[atari,box2d]\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import functools\n",
    "import time\n",
    "import gin\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadff4c0-2085-4347-bd74-2290638a8d01",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20658363-5572-4833-9977-73340c26fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "#env_name = 'CartPole-v1'\n",
    "random_seed = 42\n",
    "\n",
    "ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# Params for train\n",
    "max_episodes = 500\n",
    "max_replay = 10\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon_min = 0.01\n",
    "learning_rate = 0.005\n",
    "target_update_rate = 2\n",
    "#loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "#activation_fn = \"softmax\"\n",
    "activation_fn = \"linear\"\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035c5bb-4c86-4275-bcb1-f9612983b1bf",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1299d7-3fd4-4326-83b0-8066c8ad4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, batch_size=32, capacity=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*minibatch))\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c4348-892f-4fdd-b23d-90edca56e8e8",
   "metadata": {},
   "source": [
    "## Dueling Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba3233-19c3-47f4-83d3-1f9c463ab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    def __init__(self, network_name, state_dim, action_dim, loss_fn=keras.losses.mean_squared_error, activation_fn=\"linear\", gamma=0.99, learning_rate=0.005, max_episodes=500, eps_min=0.01):\n",
    "        self.network_name = network_name\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate\n",
    "        self.eps_den = (max_episodes * 0.85)\n",
    "        self.eps_min = eps_min\n",
    "        \n",
    "        self.checkpoint_folder = os.path.join(ROOT_DIR, \"training/\" + self.network_name)\n",
    "        os.makedirs(self.checkpoint_folder, exist_ok=True)\n",
    "        self.checkpoint_path = os.path.join(self.checkpoint_folder, 'cp-{epoch:04d}.ckpt')\n",
    "        self.checkpoint_dir = os.path.dirname(self.checkpoint_path)\n",
    "\n",
    "        self.model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        K = keras.backend\n",
    "        state_input = Input((self.state_dim,))\n",
    "        hidden1 = keras.layers.Dense(32, activation=\"relu\")(state_input)\n",
    "        hidden2 = keras.layers.Dense(32, activation=\"relu\")(hidden1)\n",
    "        state_values = keras.layers.Dense(1)(hidden2)\n",
    "        raw_advantages = keras.layers.Dense(self.action_dim)(hidden2)\n",
    "        advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "        Q_values = state_values + advantages\n",
    "        model = tf.keras.Model(state_input, Q_values)\n",
    "        model.compile(loss=loss_fn, optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "    \n",
    "    def epsilon(self, episode):\n",
    "        eps = max(1 - episode / self.eps_den, self.eps_min)\n",
    "        return eps\n",
    "    \n",
    "    def epsilon_greedy(self, state, episode):\n",
    "        if random.uniform(0,1) < self.epsilon(episode):\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            Q_value = self.predict(state)[0]\n",
    "            action = np.argmax(Q_value)\n",
    "        return action\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def get_action(self, state, episode):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        return self.epsilon_greedy(state, episode)\n",
    "    \n",
    "    def train(self, states, targets):\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "    \n",
    "    def load(self, weights=''):\n",
    "        if weights:\n",
    "             self.model.load_weights(weights)\n",
    "        else:\n",
    "            latest = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "            if latest is not None:\n",
    "                self.model.load_weights(latest)\n",
    "\n",
    "    def save(self, e):\n",
    "        self.model.save_weights(self.checkpoint_path.format(epoch=e))\n",
    "    \n",
    "    def clean(self):\n",
    "        for f in os.listdir(self.checkpoint_folder):\n",
    "            os.remove(os.path.join(self.checkpoint_folder, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a9f88-1f90-4b11-9d94-853790b889c4",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c8c7f-b0e4-4507-916f-39a9de94e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env_name, random_seed, loss_fn, activation_fn, gamma=0.95, learning_rate=0.005, max_episodes=500, eps_min=0.01, batch_size=32, max_replay=10, target_update_rate=50):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(random_seed)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_rate = target_update_rate\n",
    "        self.frames = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.image_name = env_name + \".gif\"\n",
    "        self.image_path = os.path.join(IMAGES_PATH, self.image_name)\n",
    "        \n",
    "        self.main_model = DDQN(\"main\", self.state_dim, self.action_dim, loss_fn, activation_fn, self.gamma, learning_rate, max_episodes, eps_min)\n",
    "        self.target_model = DDQN(\"target\", self.state_dim, self.action_dim, loss_fn, activation_fn, self.gamma, learning_rate, max_episodes, eps_min)\n",
    "        self.target_update()\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.batch_size)\n",
    "        \n",
    "    def cp_init(self):\n",
    "        self.main_model.clean()\n",
    "        self.target_model.clean()\n",
    "        \n",
    "    def convert_time(self, t):\n",
    "        hours, rem = divmod(t, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        return hours, minutes, seconds\n",
    "    \n",
    "    def make_gif(self):\n",
    "        frame_images = [PIL.Image.fromarray(frame) for frame in self.frames]\n",
    "        frame_images[0].save(self.image_path, format='GIF', append_images=frame_images[1:], save_all=True, duration=120, loop=0)\n",
    "    \n",
    "    def show_gif(self):\n",
    "        html_img = '<img src=' + self.image_path + ' />'\n",
    "        display.HTML(html_img)\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(self.rewards)\n",
    "        plt.xlabel(\"Episode\", fontsize=14)\n",
    "        plt.ylabel(\"Reward\", fontsize=14)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def target_update(self):\n",
    "        weights = self.main_model.model.get_weights()\n",
    "        self.target_model.model.set_weights(weights)\n",
    "    \n",
    "    def replay(self, max_replay=10):\n",
    "        for _ in range(max_replay):\n",
    "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
    "            next_q_values = self.target_model.predict(next_states).max(axis=1)\n",
    "            q_values = self.main_model.predict(states)\n",
    "            q_values[range(self.batch_size), actions] = rewards + (1-done) * next_q_values * self.gamma\n",
    "            self.main_model.train(states, q_values)\n",
    "    \n",
    "    def train(self, max_episodes=1000, max_replay=10):\n",
    "        train_time, best_reward = 0, -1000\n",
    "        self.rewards = []\n",
    "        avg_rewards = []\n",
    "        self.main_model.load()\n",
    "        self.main_model.clean()\n",
    "        for ep in range(max_episodes):\n",
    "            done, total_reward, time_step = False, 0, 0\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.main_model.get_action(state, ep)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, reward, next_state, done)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                time_step += 1\n",
    "            self.rewards.append(total_reward)\n",
    "            avg_reward = np.mean(self.rewards[-100:])\n",
    "            avg_rewards.append(avg_reward)\n",
    "            if total_reward >= best_reward:\n",
    "                best_weights = self.main_model.model.get_weights()\n",
    "                best_reward = total_reward\n",
    "                self.main_model.save(ep)\n",
    "            if self.buffer.size() >= self.batch_size:\n",
    "                start_time = time.time()\n",
    "                self.replay(max_replay)\n",
    "                train_time += time.time() - start_time\n",
    "                if ep % self.target_update_rate:\n",
    "                    self.target_update()\n",
    "            hours, minutes, seconds = self.convert_time(train_time)\n",
    "            print(\"\\rEpisode: {}, Timestep: {}, Current Reward: {:.5f}, Best Reward: {:.5f}, Avg. Reward: {:.5f}, Train Time: {:0>2}:{:0>2}:{:05.2f}\".format(ep, time_step, total_reward, best_reward, avg_reward, int(hours), int(minutes), seconds), end=\"\")\n",
    "        self.main_model.model.set_weights(best_weights)\n",
    "        self.main_model.save(ep)\n",
    "\n",
    "    def play(self):\n",
    "        state = self.env.reset()\n",
    "        self.frames = []\n",
    "        total_reward, step_indx = 0, 0\n",
    "        while True:\n",
    "            self.frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            Q_values = self.main_model.predict(state[np.newaxis])\n",
    "            action = np.argmax(Q_values[0])\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            step_indx += 1\n",
    "            print(\"\\rTimestep: {}, Reward: {:.5f}\".format(step_indx, total_reward), end=\"\")\n",
    "            if done:\n",
    "                break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b86fcd-57c9-4ae8-824c-7ab14624e263",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cefe87-2c34-4ec9-9dd6-9d1444e9a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env_name, random_seed, loss_fn, activation_fn, gamma, learning_rate, max_episodes, epsilon_min, batch_size, max_replay, target_update_rate)\n",
    "agent.cp_init()\n",
    "agent.train(max_episodes, max_replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac671c-fcd2-4d9e-8625-0efac998b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(max_episodes, max_replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a9d42-ff99-4c19-8f1f-35016233bcdd",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e4e35-550d-4e20-b9b1-ad28250e1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba3b34-4197-4c98-9198-055673e2c551",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059f38a-2ce4-4081-8bf8-3d0d63bdfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3436af-2751-44b1-9f02-19c87e44ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.make_gif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023adca-e1a3-4c43-b6cf-d6708c60c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.show_gif()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
